# -*- coding: utf-8 -*-
"""Capacitive_Sensor_Model_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12GZU4AruKO1bobZQJ9wLMWROMySYBF9N

## Configure Defaults
"""

# Define paths to model files
import os
MODELS_DIR = 'models/'
if not os.path.exists(MODELS_DIR):
    os.mkdir(MODELS_DIR)
MODEL_TF = MODELS_DIR + 'model'
MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'
MODEL_TFLITE = MODELS_DIR + 'model.tflite'
MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'

"""## Setup Environment

Install Dependencies
"""

! pip install tensorflow==2.4.0

"""Import Dependencies"""

# TensorFlow is an open source machine learning library
import tensorflow as tf

# Keras is TensorFlow's high-level API for deep learning
from tensorflow import keras
# Numpy is a math library
import numpy as np
# Pandas is a data manipulation library 
import pandas as pd
# Matplotlib is a graphing library
import matplotlib.pyplot as plt
# Math is Python's math library
import math

# Set seed for experiment reproducibility
seed = 1
np.random.seed(seed)
tf.random.set_seed(seed)

"""## Dataset"""



from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/Shareddrives/EE599_project/sensor_data_new.csv', header=None)
df = df.sample(frac=1)

# df = df.apply(pd.to_numeric)
feature_col_ids = list(range(0,36))

x_values = df[feature_col_ids].astype(int, errors='ignore')

y_values = df[36].astype(int, errors='ignore')
SAMPLES = df.shape[0]

df[[36]].value_counts()

"""### 3. Split the Data

The data is split as follows:
  1. Training: 60%
  2. Validation: 20%
  3. Testing: 20% 


"""

# We'll use 60% of our data for training and 20% for testing. The remaining 20%
# will be used for validation. Calculate the indices of each section.
TRAIN_SPLIT =  int(0.6 * SAMPLES)
TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)

# Use np.split to chop our data into three parts.
# The second argument to np.split is an array of indices where the data will be
# split. We provide two indices, so the data will be divided into three chunks.
x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])
y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])

"""## Training

### 1. Design the Model
"""

y_test

y_values

x_train

x_test

# We'll use Keras to create a simple model architecture
model_1 = tf.keras.Sequential()
model_1.add(keras.layers.Dense(16, activation='relu', input_shape=(36,)))
# model_1.add(keras.layers.Dense(10, activation='relu'))
# model_1.add(keras.layers.Dense(8, activation='relu'))
# model_1.add(keras.layers.Dense(6, activation='relu'))
model_1.add(keras.layers.Dense(3, activation='relu'))

model_1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
print(model_1.summary())

"""### 2. Train the Model

"""

y_validate.value_counts()



# Train the model on our training data while validating on our validation set
history_1 = model_1.fit(x_train, y_train, epochs=30, batch_size=16,
                        validation_data=(x_validate, y_validate))
model_1.save(MODEL_TF)

history_1.history

"""### 3. Plot Metrics"""

# Draw a graph of the loss, which is the distance between
# the predicted and actual values during training and validation.
train_loss = history_1.history['loss']
val_loss = history_1.history['val_loss']

epochs = range(1, len(train_loss) + 1)

plt.plot(epochs, train_loss, 'g.', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Exclude the first few epochs so the graph is easier to read
SKIP = 2

plt.plot(epochs[SKIP:], train_loss[SKIP:], 'g.', label='Training loss')
plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.clf()

# Draw a graph of mean absolute error, which is another way of
# measuring the amount of error in the prediction.
train_acc = history_1.history['accuracy']
val_acc = history_1.history['val_accuracy']

plt.plot(epochs[SKIP:], train_acc[SKIP:], 'g.', label='Training Acc')
plt.plot(epochs[SKIP:], val_acc[SKIP:], 'b.', label='Validation Acc')
plt.title('Training and validation Acc')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

y_test.value_counts()

from tensorflow.keras.models import load_model

m = load_model(MODEL_TF)
model_1 = m

# Calculate and print the loss on our test dataset
test_loss, test_acc = model_1.evaluate(x_test, y_test)

# Make predictions based on our test dataset
y_test_pred = np.argmax(model_1.predict(x_test), axis=1)

# # Graph the predictions against the actual values
# plt.clf()
# plt.title('Comparison of predictions and actual values')
# plt.plot(x_test, y_test, 'b.', label='Actual values')
# # plt.plot(x_test, y_test, 'b.')
# plt.plot(x_test, y_test_pred, 'r.', label='TF predictions')
# plt.legend()
# plt.show()

pd.DataFrame(y_test_pred).value_counts()

pd.DataFrame(y_test).value_counts()

test_acc

"""### 1. Generate Models without Quantization

"""

# Convert the model to the TensorFlow Lite format without quantization
converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_TF)
model_no_quant_tflite = converter.convert()

# Save the model to disk
open(MODEL_NO_QUANT_TFLITE, "wb").write(model_no_quant_tflite)

# # Convert the model to the TensorFlow Lite format with quantization
# def representative_dataset():
#   for i in range(500):
# #    yield([x_train[i].values.reshape(35, 1)])
#     yield(x_train[i])
# # Set the optimization flag.
# converter.optimizations = [tf.lite.Optimize.DEFAULT]
# # Enforce integer only quantization
# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# converter.inference_input_type = tf.int8
# converter.inference_output_type = tf.int8
# # Provide a representative dataset to ensure we quantize correctly.
# converter.representative_dataset = representative_dataset
# model_tflite = converter.convert()

# # Save the model to disk
# open(MODEL_TFLITE, "wb").write(model_tflite)

# Calculate size
size_tf = os.path.getsize(MODEL_TF)
size_no_quant_tflite = os.path.getsize(MODEL_NO_QUANT_TFLITE)
# size_tflite = os.path.getsize(MODEL_TFLITE)
print(size_tf)
print(size_no_quant_tflite)

"""**Summary**

## Generate a TensorFlow Lite for Microcontrollers Model
Convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers.
"""

# Install xxd if it is not available
!apt-get update && apt-get -qq install xxd
# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model
!xxd -i {MODEL_NO_QUANT_TFLITE} > {MODEL_TFLITE_MICRO}
# Update variable names
REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')
!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}

"""## Deploy to a Microcontroller

"""

# Print the C source file
!cat {MODEL_TFLITE_MICRO}

